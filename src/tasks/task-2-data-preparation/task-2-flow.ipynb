{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install and import packages","metadata":{"id":"bxkWTVD1vsa2"}},{"cell_type":"code","source":"!pip install sentence-transformers","metadata":{"id":"c5DAw04s9UrB","execution":{"iopub.status.busy":"2022-11-01T09:48:52.597793Z","iopub.execute_input":"2022-11-01T09:48:52.598202Z","iopub.status.idle":"2022-11-01T09:49:02.921033Z","shell.execute_reply.started":"2022-11-01T09:48:52.598159Z","shell.execute_reply":"2022-11-01T09:49:02.919676Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: sentence-transformers in /opt/conda/lib/python3.7/site-packages (2.2.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.21.6)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.12.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.0.2)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.1.97)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.11.0)\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.20.1)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (3.7)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.3)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.10.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.64.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.13.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.11.10)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (8.0.4)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (9.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.12)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport sklearn.feature_extraction.text as txt\nimport bs4\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport datetime\nimport re\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport math\nimport time\nimport transformers\nfrom transformers import pipeline","metadata":{"id":"DeI9glIi9UrD","execution":{"iopub.status.busy":"2022-11-01T09:49:02.925807Z","iopub.execute_input":"2022-11-01T09:49:02.926192Z","iopub.status.idle":"2022-11-01T09:49:02.933996Z","shell.execute_reply.started":"2022-11-01T09:49:02.926147Z","shell.execute_reply":"2022-11-01T09:49:02.932910Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# Function to read data","metadata":{"id":"BB-lHCsx1aAQ"}},{"cell_type":"code","source":"def verify_data_structure(df):\n    \n    cols = set(['Soft Skill Name', 'Criteria', 'URL', 'Title of URL', 'Content'])\n    \n    if not isinstance(df, pd.DataFrame):\n        raise Exception(\"df must be a dataframe that contains these columns : \" + str(cols))\n        \n    if cols.issubset(set(list(df.columns))):\n        return df\n    else:\n        raise Exception(\"incorrect dataframe structure ! - expect these columns : \" + str(cols))\n        \ndef read_data(path_):\n    data = pd.read_csv(path_)\n    \n    # verify data is in the required form\n    return verify_data_structure(data)        \n","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:04:53.961997Z","iopub.execute_input":"2022-11-01T10:04:53.962687Z","iopub.status.idle":"2022-11-01T10:04:53.969696Z","shell.execute_reply.started":"2022-11-01T10:04:53.962648Z","shell.execute_reply":"2022-11-01T10:04:53.968226Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"# Function for pre-processing.  for validation","metadata":{"id":"r_qQ-XNu9UrI"}},{"cell_type":"code","source":"def pre_processing_data(data, min_paragraphs_words = 10):\n    \n    print('----> data pre-processing - initial shape of data : ', data.shape)\n    \n    # clean paragraph\n    data['Title of URL'] = data['Title of URL'].apply(lambda x : re.sub(\"[\\s\\n\\t\\b\\']+\",\" \", str(x)).strip())\n\n    # remove duplicate URLs\n    data = data.drop_duplicates('URL')\n    \n    print('----> data pre-processing - shape of data after duplicate removal : ', data.shape)\n\n    # length of title and content\n    data['length_of_title'] = data['Title of URL'].apply(len)\n    data['length_of_content'] = data['Content'].apply(len)\n\n    # number of words (space separeted)\n    def n_words(text):\n        return len(text.split(' '))\n\n    data['n_words_title'] = data['Title of URL'].apply(n_words)\n    data['n_words_content'] = data['Content'].apply(n_words)\n\n  # extract paragraphs from Content\n    def extract_paragraphs(content):\n        paragraph = ''\n        soup = bs4.BeautifulSoup(content, \"html.parser\")\n        paragraphs = soup.find_all('p')\n        for p in paragraphs:\n            paragraph = paragraph + p.get_text()\n        return paragraph\n\n    data['paragraphs'] = data['Content'].apply(extract_paragraphs)\n    data['length_of_paragraphs'] = data['paragraphs'].apply(len)\n\n    # clean paragraphs\n    data['paragraphs'] = data['paragraphs'].apply(lambda x : re.sub(\"[\\s\\n\\t\\b\\']+\",\" \", str(x)).strip())\n    \n    # number of words in paragraphs\n    data['n_words_paragraphs'] = data['paragraphs'].apply(n_words)\n    \n    # take paragraphs that has a min number of words\n    data = data.loc[data['n_words_paragraphs'] > min_paragraphs_words , :]\n    \n    print('----> data pre-processing - shape of data after min words paragraphs removal : ', data.shape)\n\n    #Stats number of words\n    data[['n_words_title', 'n_words_paragraphs']].describe().T\n\n    return data","metadata":{"id":"WEc7xQOZ-EOd","execution":{"iopub.status.busy":"2022-11-01T09:49:02.947373Z","iopub.execute_input":"2022-11-01T09:49:02.947747Z","iopub.status.idle":"2022-11-01T09:49:02.962561Z","shell.execute_reply.started":"2022-11-01T09:49:02.947706Z","shell.execute_reply":"2022-11-01T09:49:02.961468Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# search top n most frequent words\ndef get_top_n_frequent_word(content, n = 1):\n    \n    stop_words_ = txt.ENGLISH_STOP_WORDS\n    count_vect = CountVectorizer(stop_words = 'english') # TODO : test ...stop_words = stop_words_)\n    \n    # get the matrix for words count\n    corpus = [content] # corpus must be a list that's why we do this\n    try:\n        X_words_counts = count_vect.fit_transform(corpus)\n    except :\n        return '', 0\n\n    words = []\n    counts = []\n    for word in count_vect.vocabulary_:\n        try:\n            i = count_vect.vocabulary_[word] # get the index of the word in the matrix of words\n            words.append(word)\n            counts.append(X_words_counts[0,i])\n        except:\n            words.append('')\n            counts.append(0)\n            continue\n            \n\n    counts_df = pd.DataFrame.from_dict({'word' : words, 'count':counts})\n    counts_df = counts_df.sort_values(by='count', ascending = False)\n    counts_df = counts_df.head(n).reset_index().drop('index', axis=1)\n    \n    words = list(counts_df['word'])\n    counts = list(counts_df['count'])\n    \n    return words[-1], counts[-1]","metadata":{"id":"sLSl_uk59UrO","execution":{"iopub.status.busy":"2022-11-01T09:49:02.966404Z","iopub.execute_input":"2022-11-01T09:49:02.967211Z","iopub.status.idle":"2022-11-01T09:49:02.978923Z","shell.execute_reply.started":"2022-11-01T09:49:02.967161Z","shell.execute_reply":"2022-11-01T09:49:02.977919Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# most frequent word \ndef get_most_frequent_word(content):\n    return get_top_n_frequent_word(content, 1)[0]\n\ndef get_most_frequent_word_count(content):\n    return get_top_n_frequent_word(content, 1)[1]\n    ","metadata":{"id":"iz1LiT3a9UrO","execution":{"iopub.status.busy":"2022-11-01T09:49:02.980745Z","iopub.execute_input":"2022-11-01T09:49:02.981505Z","iopub.status.idle":"2022-11-01T09:49:02.992969Z","shell.execute_reply.started":"2022-11-01T09:49:02.981469Z","shell.execute_reply":"2022-11-01T09:49:02.991949Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def words_frequency(data, col = 'paragraphs'):\n    data[col + '_most_frequent_word'] = data[col].apply(get_most_frequent_word)\n    data[col + '_most_frequent_word_count'] = data[col].apply(get_most_frequent_word_count)\n\n    return data\n","metadata":{"id":"fgQGpbMaEBtj","execution":{"iopub.status.busy":"2022-11-01T09:49:02.994463Z","iopub.execute_input":"2022-11-01T09:49:02.995094Z","iopub.status.idle":"2022-11-01T09:49:03.003521Z","shell.execute_reply.started":"2022-11-01T09:49:02.995042Z","shell.execute_reply":"2022-11-01T09:49:03.002497Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# Disaggregating articles' content into headings and their respective paragraphs","metadata":{"id":"N9DBlNhE9UrR"}},{"cell_type":"code","source":"# this function disaggregates the content column, which is a big set of paragraphs into \n# many smaller paragraphs\n\n#**************************************************************************************\n# to save memory the Content and paragraphs will be droped from the returned dataframe\n#**************************************************************************************\n            \ndef disaggregate_content_medium(df): \n    # these list are used to constract the returned dataframe\n    soft_kill_names_df = [] # \n    criterias_df = [] \n    URLs_df = []\n    titles_of_URLs_df = []\n    summaries_df = [] # summary produced in the previous step (summary of all the concateneted paragraphs)\n    paragraphs_df = []\n    headings_df = [] # heading just before paragraph\n    headings_type_df = [] # type of tytle : h1, h2,...\n    \n\n    for idx, row in df.iterrows():\n        \n        paragraphs = row['Content'].split('\\n')\n        \n        for paragraph in paragraphs:\n            \n            \n            if paragraph == '': # if empty we continue\n                continue\n            # get the old columns of df except Content beacause it's huge and we won't need it later\n            soft_kill_names_df.append(row['Soft Skill Name']) \n            criterias_df.append(row['Criteria']) \n            URLs_df.append(row['URL'])\n            titles_of_URLs_df.append(row['Title of URL'])\n            summaries_df.append('no summary')#summaries_df.append(row['summary'])\n        \n    disaggregated_df = pd.DataFrame.from_dict({'Soft Skill Name' : soft_kill_names_df, \n                                               'Criteria' : criterias_df, \n                                               'URL': URLs_df, \n                                               'Title of URL' : titles_of_URLs_df, \n                                               'summary of Content' : summaries_df,\n                                               'paragraph' : paragraphs_df})\n    disaggregated_df['header'] = ''\n    return disaggregated_df\n","metadata":{"id":"_DBGfA7J9UrR","execution":{"iopub.status.busy":"2022-11-01T09:49:03.005097Z","iopub.execute_input":"2022-11-01T09:49:03.005742Z","iopub.status.idle":"2022-11-01T09:49:03.016408Z","shell.execute_reply.started":"2022-11-01T09:49:03.005705Z","shell.execute_reply":"2022-11-01T09:49:03.015577Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# this function disaggregates the content column (html format) of the df dataframe into many paragraphs\n# For each paragraph, it searchs for the first previous heading found\n\n#**************************************************************************************\n# to save memory the Content and paragraphs will be droped from the returned dataframe\n#**************************************************************************************\n\ndef disaggregate_content(df, tag = 'p', a_class = None): # df is supposed a dataframe that contains the Content column in the html format : <body>....</body>\n    # these list are used to constract the returned dataframe\n    soft_kill_names_df = [] # \n    criterias_df = [] \n    URLs_df = []\n    titles_of_URLs_df = []\n    summaries_df = [] # summary produced in the previous step (summary of all the concateneted paragraphs)\n    paragraphs_df = []\n    headings_df = [] # heading just before paragraph\n    headings_type_df = [] # type of tytle : h1, h2,...\n    \n\n    for idx, row in df.iterrows():\n        soup = bs4.BeautifulSoup(row['Content'], \"html.parser\")\n        \n        if a_class != None:\n            paragraphs = soup.find_all(tag, a_class) \n        else:\n            paragraphs = soup.find_all(tag) \n            \n        for paragraph in paragraphs:\n            \n            # get paragraph text\n            temp = paragraph.get_text()\n            if temp == '': # if empty we continue\n                continue\n            \n            paragraphs_df.append(paragraph.get_text())\n            \n            # get the old columns of df except Content beacause it's huge and we won't need it later\n            soft_kill_names_df.append(row['Soft Skill Name']) \n            criterias_df.append(row['Criteria']) \n            URLs_df.append(row['URL'])\n            titles_of_URLs_df.append(row['Title of URL'])\n            summaries_df.append('no summary')#summaries_df.append(row['summary'])\n            \n            # find heading just before that paragraph. find_heading returns (title, hx). See definition below\n            heading = find_heading(paragraph)\n            headings_df.append(heading[0]) # the text heading\n            try:\n                # it's type : h1...h6\n                headings_type_df.append(heading[1])\n            except:\n                # if no heading found affect None\n                headings_type_df.append('None')\n                continue\n        \n    \n    disaggregated_df = pd.DataFrame.from_dict({'Soft Skill Name' : soft_kill_names_df, \n                                               'Criteria' : criterias_df, \n                                               'URL': URLs_df, \n                                               'Title of URL' : titles_of_URLs_df, \n                                               'summary of Content' : summaries_df,\n                                               'header':headings_df,\n                                                'paragraph' : paragraphs_df})\n    \n    \n    # clean paragraph\n    disaggregated_df['paragraph'] = disaggregated_df['paragraph'].apply(lambda x : re.sub(\"[\\s\\n\\t\\b\\']+\",\" \", str(x)).strip())\n\n    disaggregated_df = disaggregated_df.drop_duplicates('paragraph')\n    \n    return disaggregated_df\n\n\n\n# this function looks for the first heading previous to elt passed as parameter\n# starting form h6 until h1\ndef find_heading(elt):\n    headings = ['h6', 'h5', 'h4', 'h3', 'h2', 'h1']\n    elt = elt.previous_element\n    while (not (elt is None) ) and (not (elt.name in headings)):\n         elt = elt.previous_element\n    \n    if (not (elt is None) ):\n        return elt.get_text(), elt.name\n    else:\n        return 'None', 'None'\n","metadata":{"id":"nFqggI9g9UrS","execution":{"iopub.status.busy":"2022-11-01T09:49:03.019985Z","iopub.execute_input":"2022-11-01T09:49:03.020353Z","iopub.status.idle":"2022-11-01T09:49:03.036207Z","shell.execute_reply.started":"2022-11-01T09:49:03.020326Z","shell.execute_reply":"2022-11-01T09:49:03.035373Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# scraped_data_disagg = disaggregate_content(scraped_data, tag='div', a_class={\"class\":\"step\"})\n# scraped_data_disagg = disaggregate_content(scraped_data, tag='p')","metadata":{"id":"tXrHUGor9UrT","execution":{"iopub.status.busy":"2022-11-01T09:49:03.039743Z","iopub.execute_input":"2022-11-01T09:49:03.040109Z","iopub.status.idle":"2022-11-01T09:49:03.050904Z","shell.execute_reply.started":"2022-11-01T09:49:03.040082Z","shell.execute_reply":"2022-11-01T09:49:03.049904Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"# Similarity Functions","metadata":{"id":"1rvXTFqMyQOz"}},{"cell_type":"code","source":"#Cosine similarity function\ndef cos_sim(p1, p2, model):\n#     model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n    p1_embed = model.encode(p1)\n    p2_embed = model.encode(p2)\n    similarity = cosine_similarity([p1_embed], [p2_embed])\n    return similarity[0]\n\n\n#Calculate similarity with respect to a column\ndef paragraph_similarity_col(dataframe, col, model):\n    \n    assert col in list(dataframe.columns), col + ' not present in dataframe !'\n    \n    url_col = dataframe[col].unique()                                                                \n    similarity = []                                              \n\n    for i in range(0, len(url_col)):\n        content_url_col = dataframe[dataframe[col] == url_col[i]]['paragraph']           \n        idx = content_url_col.index                                                                    \n\n        for j in idx:                                                                  \n            sim = cos_sim(url_col[i], content_url_col[j], model)[0]                  \n            similarity.append(sim)\n\n    dataframe['similarity_wrt_' + col] = similarity\n\n    return dataframe\n\n#calculate similarity with respect to reference paragraph\ndef similarity_wrt_ref_par(dataframe, model):\n    url_titles = dataframe['Title of URL'].unique() \n    simi = []                                                                     \n\n    for i in range(0, len(url_titles)):\n        dft = dataframe[dataframe['Title of URL'] == url_titles[i]]                \n        max_similar = dft['similarity_wrt_title'].max()                            \n        ref_para = dft[dft['similarity_wrt_title'] == max_similar]['paragraph']    \n        ref_index = ref_para.index[0]                                        \n        paragraph = dft['paragraph']\n        idx = paragraph.index\n\n    for i in idx:\n        sim = cos_sim(ref_para[ref_index], paragraph[i], model)[0]                       \n        simi.append(sim)\n\n    dataframe['similarity_wrt_ref_para'] = simi\n\n    return dataframe\n\n\n\n#calculate the difference between two consecutive paragraphs\ndef difference(dataframe, diff_treshold = 0.05):\n    url_titles = dataframe['Title of URL'].unique()\n    index = []                                                 \n\n    for i in range(0, len(url_titles)):\n        dft = dataframe[dataframe['Title of URL'] == url_titles[i]]                 \n        simil = dft['similarity_wrt_ref_para']\n        idx = simil.index\n\n    for i in range(0, len(idx) - 2):                                    \n        diff = abs(simil[idx[i]] - simil[idx[i+1]])                       \n        if diff <  diff_treshold:\n            index.append(idx[i+1])    \n\n    return dataframe.drop(indices, axis = 0)\n\n  ","metadata":{"id":"t5E1AL3ASec-","execution":{"iopub.status.busy":"2022-11-01T09:49:03.053231Z","iopub.execute_input":"2022-11-01T09:49:03.054004Z","iopub.status.idle":"2022-11-01T09:49:03.068321Z","shell.execute_reply.started":"2022-11-01T09:49:03.053970Z","shell.execute_reply":"2022-11-01T09:49:03.067446Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# Zero shot classification","metadata":{"id":"uzinhtGd9UrV"}},{"cell_type":"code","source":"def zs_classify(data, candidate_labels):\n\n    text_list = list(data['paragraph'])\n\n    zeroshot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n\n    labels = []\n    scores = []\n    for t in text_list:\n        class_ = zeroshot_classifier(t, candidate_labels, multi_label=False)\n        labels.append(class_['labels'][0])\n        scores.append(class_['scores'][0])\n\n    data['zs_class'] = labels\n    data['zs_class_score'] = scores\n\n    return data","metadata":{"id":"EPCQ_QQD9UrV","execution":{"iopub.status.busy":"2022-11-01T09:49:03.071542Z","iopub.execute_input":"2022-11-01T09:49:03.071792Z","iopub.status.idle":"2022-11-01T09:49:03.084645Z","shell.execute_reply.started":"2022-11-01T09:49:03.071768Z","shell.execute_reply":"2022-11-01T09:49:03.083693Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"# Stats Functions","metadata":{"id":"sFyl6XMzyhQ_"}},{"cell_type":"code","source":"def paragraph_stats(dataframe):\n    dataframe['n_words_paragraph'] = dataframe['paragraph'].apply(n_words)\n    dataframe['most_frequent_word'] = dataframe['paragraph'].apply(get_most_frequent_word)\n    dataframe['most_frequent_word_count'] = dataframe['paragraph'].apply(get_most_frequent_word_count)\n\n    return dataframe\n","metadata":{"id":"7H2R-Je2vEcB","execution":{"iopub.status.busy":"2022-11-01T09:49:03.087013Z","iopub.execute_input":"2022-11-01T09:49:03.087863Z","iopub.status.idle":"2022-11-01T09:49:03.095093Z","shell.execute_reply.started":"2022-11-01T09:49:03.087836Z","shell.execute_reply":"2022-11-01T09:49:03.094219Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# scraped data is a dataframe that has the \ndef task_2_pipeline(data, forum = '', ZSC_labels = None):\n    \n    # verify structure of data - this line raises Exception if structure not valid\n    scraped_data = verify_data_structure(data)\n    \n    # init the similarity model\n    model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n    \n    # pre-process data - concat Content columns and create paragraphs columns \n    scraped_data = pre_processing_data(scraped_data, min_paragraphs_words = 10)\n    scraped_data = words_frequency(scraped_data, col = 'paragraphs')\n    \n    # disaggregation depends on the forum !\n    if forum.lower() == 'wikihow':\n        scraped_data = disaggregate_content(scraped_data, tag='div', a_class={\"class\":\"step\"})\n    elif forum.lower() == 'medium':\n        scraped_data = disaggregate_content_medium(scraped_data)\n    else:\n        scraped_data = disaggregate_content(scraped_data, tag='p')\n    \n    # create similarty columns : similarity of paragraph to title and Soft skill name\n    scraped_data = paragraph_similarity_col(scraped_data, 'Title of URL', model)\n    scraped_data = paragraph_similarity_col(scraped_data, 'Soft Skill Name', model)\n    \n    # Zero-Shot classication - labels : Soft skills\n    if ZSC_labels != None:\n        scraped_data = zs_classify(scraped_data, ZSC_labels)\n        print('shape of data after ZSC : ', scraped_data.shape)\n    \n    return scraped_data","metadata":{"execution":{"iopub.status.busy":"2022-11-01T09:49:03.100034Z","iopub.execute_input":"2022-11-01T09:49:03.100342Z","iopub.status.idle":"2022-11-01T09:49:03.110691Z","shell.execute_reply.started":"2022-11-01T09:49:03.100317Z","shell.execute_reply":"2022-11-01T09:49:03.109718Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"# Run the Flow","metadata":{"id":"ODeAvXAM1N-a"}},{"cell_type":"code","source":"if __name__ == \"__main__\": \n    # Read the skills if we want to run ZSC \n    skills = pd.read_excel('../input/softskillsv3/skills_df_v3.xlsx')\n    candidate_labels = list(skills['Skills category'].unique())\n    \n    # read data\n    data = pd.read_csv('../input/wikihow-20221029/wiki_how_20221029.csv')\n    \n    # Run the flow - put ZSC_labels at None if you don't want to run ZSC\n    disaggregated_data = task_2_pipeline(data, 'wikihow', ZSC_labels=None )\n    \n    # Export result\n    disaggregated_data.to_csv('disaggregated_data_20221101.csv', index = False)\nelse: \n    print (\"task-2-flow imported !\")","metadata":{"id":"PplWLQZo1RPf","execution":{"iopub.status.busy":"2022-11-01T09:49:03.113851Z","iopub.execute_input":"2022-11-01T09:49:03.114554Z","iopub.status.idle":"2022-11-01T09:49:07.098075Z","shell.execute_reply.started":"2022-11-01T09:49:03.114528Z","shell.execute_reply":"2022-11-01T09:49:07.097036Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"shape of data :  (3698, 5)\n----> data pre-processing - initial shape of data :  (3698, 5)\n----> data pre-processing - shape of data after duplicate removal :  (125, 5)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","output_type":"stream"},{"name":"stdout","text":"----> data pre-processing - shape of data after min words paragraphs removal :  (3, 12)\nshape of data after pre-processing :  (3, 14)\nshape of data after disaggregation :  (3, 7)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d0ec70116234a5886edaf0f2caa893d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"658b3b840aa243709a526bdb3f4f356e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"deb3e7a90b41444f83eff37bc27898ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46eccceec477479ba11f6f86ad445d41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb160e2ffffd4edaae8b441035c32cfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aba4fadc87084040b8c7d506162f9551"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f914d1c3d58b43acb4953f9509041133"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e410a9eb293e4755b2236e06f602429b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0e9258523bc41629ab88cfab47e19ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55a02ba3352c432b8a63dde967f8f698"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba1ee972044749478d5e5592bbbd5bca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c621055b9f264663972d845afa36ee7e"}},"metadata":{}},{"name":"stdout","text":"shape of data after similarity calculations :  (3, 9)\n","output_type":"stream"}]}]}